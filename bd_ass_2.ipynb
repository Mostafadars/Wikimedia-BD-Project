{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ***Install the PySpark Package***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8McmTDZwadDQ",
        "outputId": "b1257e0e-056d-4c13-f1e9-64ede30ca426"
      },
      "outputs": [],
      "source": [
        "#pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Run This if you work on the Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5difnAVaieJ",
        "outputId": "e38bd2fc-a639-4a56-db22-1a701e090e32"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ***Import Spark Modules***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-u50JqnZaowR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark import SparkContext\n",
        "import time\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ***Initialize The Spark Session and Read the Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "_jovBJeXanmZ"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "sc = SparkContext(appName=\"WikimediaPageViews\")\n",
        "\n",
        "# Uncomment the line below if you work on google colab\n",
        "# Example path: \"/content/drive/MyDrive/Big Data Ass 2/pagecounts-20160101-000000_parsed.out\"\n",
        "# data_path = \"put-your-content-drive-path-to-the-input-file-here/pagecounts-20160101-000000_parsed.out\"\n",
        "\n",
        "# Uncomment the line below if you work on local\n",
        "data_path = \"./pagecounts-20160101-000000_parsed.out\"\n",
        "\n",
        "lines = sc.textFile(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and parse the data\n",
        "def parse_line(line):\n",
        "    parts = line.strip().split(' ')\n",
        "    if len(parts) < 4:\n",
        "        return None\n",
        "    project, title, hits, size = parts[0], parts[1], int(parts[2]), int(parts[3])\n",
        "    return (project, title, hits, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed = lines.map(parse_line).filter(lambda x: x is not None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssJtS_eTazdp"
      },
      "source": [
        "# ***Function 1: Compute min, max, and average page size***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_1 - Spark Map Reduce\n",
        "\n",
        "def function_1_map_reduce(parsed_rdd):\n",
        "    start = time.time()\n",
        "    print(f\"Start time: {start}\")\n",
        "    \n",
        "    # Extract page sizes\n",
        "    page_sizes = parsed_rdd.map(lambda x: x[3])  # x[3] is page size\n",
        "    \n",
        "    # MapReduce operations\n",
        "    min_size = page_sizes.min()\n",
        "    max_size = page_sizes.max()\n",
        "    \n",
        "    total_size = page_sizes.sum()\n",
        "    count = page_sizes.count()\n",
        "    avg_size = total_size / count if count > 0 else 0\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End time: {end}\")\n",
        "    \n",
        "    print(f\"Min Page Size: {min_size}\")\n",
        "    print(f\"Max Page Size: {max_size}\")\n",
        "    print(f\"Average Page Size: {avg_size:.2f}\")\n",
        "\n",
        "    \n",
        "# running function 1   \n",
        "function_1_map_reduce(parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_1 - Spark Loops\n",
        "def function_1_spark_loop(rdd):\n",
        "    print(\"\\nQ1 Spark foreach\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    sum_size = sc.accumulator(0)\n",
        "    count = sc.accumulator(0)\n",
        "\n",
        "    min_size = float('inf')\n",
        "    max_size = float('-inf')\n",
        "\n",
        "    def update_stats(x):\n",
        "        size = x[3]\n",
        "        sum_size.add(size)\n",
        "        count.add(1)\n",
        "        return size \n",
        "\n",
        "    rdd.foreach(update_stats)\n",
        "\n",
        "    sizes = rdd.map(lambda x: x[3]).collect()\n",
        "    min_size = min(sizes)\n",
        "    max_size = max(sizes)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(f\"Min page size: {min_size}\")\n",
        "    print(f\"Max page size: {max_size}\")\n",
        "    print(f\"Average page size: {sum_size.value / count.value:.2f}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "function_1_spark_loop(parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_1 - Normal Loops\n",
        "def function_1_normal_loop(rdd):\n",
        "    start_time = time.time()\n",
        "    min_page_title = float('inf')\n",
        "    max_page_title = float('-inf')\n",
        "    sum_page_title = 0\n",
        "    count = 0\n",
        "\n",
        "    for _, _, _, value in rdd.toLocalIterator():\n",
        "        if value < min_page_title:\n",
        "            min_page_title = value\n",
        "\n",
        "        if value > max_page_title:\n",
        "            max_page_title = value\n",
        "\n",
        "        sum_page_title += value\n",
        "        count += 1\n",
        "\n",
        "    avg_page_title = sum_page_title / count\n",
        "\n",
        "    print(\"Minimum page title size:\", min_page_title)\n",
        "    print(\"Maximum page title size:\", max_page_title)\n",
        "    print(\"Average page title size:\", avg_page_title)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
        "\n",
        "\n",
        "function_1_normal_loop(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMLwH3nBa4DI"
      },
      "source": [
        "# ***Function 2: Count page titles starting with \"The\" not in English project***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZRTT9mqa84A",
        "outputId": "eabe0b3e-ab9d-4872-8f86-4f26cfe3a682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total titles starting with 'The': 45020\n",
            "Titles starting with 'The' NOT in English project: 10292\n"
          ]
        }
      ],
      "source": [
        "# Function_2 - Spark Map Reduce\n",
        "def function_2_map_reduce(parsed_rdd):\n",
        "    start = time.time()\n",
        "    print(f\"Start time: {start}\")\n",
        "    \n",
        "    titles_with_the = parsed_rdd.filter(lambda x: x[1].startswith(\"The\")) # x[1] is title\n",
        "    \n",
        "    count_titles_with_the = titles_with_the.count()\n",
        "    \n",
        "    non_english_titles_with_the = titles_with_the.filter(lambda x: x[0] != \"en\") # x[0] is project code\n",
        "    \n",
        "    # Count how many non-English titles start with \"The\"\n",
        "    count_non_english = non_english_titles_with_the.count()\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End time: {end}\")\n",
        "    \n",
        "    print(f\"Total titles starting with 'The': {count_titles_with_the}\")\n",
        "    print(f\"Titles starting with 'The' and NOT in English project: {count_non_english}\")\n",
        "    \n",
        "    \n",
        "# running function 2\n",
        "function_2_map_reduce(parsed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_2 - Spark Loops\n",
        "def function_2_spark_loop(rdd):\n",
        "    print(\"\\nQ2 Spark foreach\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start:.4f}\")\n",
        "\n",
        "    total_count = sc.accumulator(0)\n",
        "    not_en_count = sc.accumulator(0)\n",
        "\n",
        "    def count_titles(x):\n",
        "        if x[1].startswith(\"The\"):\n",
        "            total_count.add(1)\n",
        "            if x[0] != \"en\":\n",
        "                not_en_count.add(1)\n",
        "\n",
        "    rdd.foreach(count_titles)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end:.4f}\")\n",
        "    print(f\"Titles starting with 'The': {total_count.value}\")\n",
        "    print(f\"Titles not in English: {not_en_count.value}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_2_spark_loop(parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_2 - Normal Loops\n",
        "def function_2_normal_loop(rdd):\n",
        "    print(\"\\nQ2 Normal Loops\")\n",
        "    total = 0\n",
        "    not_en = 0\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start:.4f}\")\n",
        "\n",
        "    for x in rdd.toLocalIterator():\n",
        "        if x[1].startswith(\"The\"):\n",
        "            total += 1\n",
        "            if x[0] != \"en\":\n",
        "                not_en += 1\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end:.4f}\")\n",
        "    print(f\"Titles starting with 'The': {total}\")\n",
        "    print(f\"Titles not in English: {not_en}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_2_normal_loop(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRsUVR6ya9LQ"
      },
      "source": [
        "# ***Function 3: Count unique terms in page titles***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rflk32-bbBi5",
        "outputId": "34b9ddc1-b784-4392-bcd5-f498bae4361a"
      },
      "outputs": [],
      "source": [
        "# Function_3 - Spark Map Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_3 - Spark Loops\n",
        "def function_3_spark_loop(rdd):\n",
        "    print(\"\\nQ3 Spark foreach\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    all_terms = rdd.flatMap(lambda x: re.split(r'_', x[1].lower())).collect()\n",
        "\n",
        "    unique_terms = set(all_terms)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(f\"Number of unique terms: {len(unique_terms)}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_3_spark_loop(parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_3 - Normal Loops\n",
        "def function_3_normal_loop(rdd):\n",
        "    print(\"\\nQ3 Normal Loops\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    terms = set()\n",
        "    for x in rdd.toLocalIterator():\n",
        "        for term in re.split(r'_', x[1].lower()):\n",
        "            terms.add(re.sub(r'[^a-zA-Z0-9]', '', term))\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(f\"Unique terms: {len(terms)}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_3_normal_loop(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0a7JSPobDHI"
      },
      "source": [
        "# ***Function 4: Extract title counts***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5luedl8bBDZ",
        "outputId": "62df0acf-4acc-464e-897d-23ac3e627d82"
      },
      "outputs": [],
      "source": [
        "# Function_4 - Spark Map Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_4 - Spark Loops\n",
        "def function_4_spark_loop(rdd):\n",
        "    print(\"\\nQ4 Spark foreach\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    title_counts = rdd.map(lambda x: (x[1], 1)).collect()\n",
        "\n",
        "    counts = {}\n",
        "    for title, count in title_counts:\n",
        "        counts[title] = counts.get(title, 0) + count\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(\"First 5 title counts:\")\n",
        "    for i, (title, count) in enumerate(list(counts.items())[:5]):\n",
        "        print(f\"{title}: {count}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_4_spark_loop(parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_4 - Normal Loops\n",
        "def function_4_normal_loop(rdd):\n",
        "    print(\"\\nQ4 Normal Loops\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    counts = {}\n",
        "    for x in rdd.toLocalIterator():\n",
        "        counts[x[1]] = counts.get(x[1], 0) + 1\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "\n",
        "    results = list(counts.items())[:5]\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(\"First 5 title counts:\")\n",
        "    for title, count in results:\n",
        "        print(f\"{title}: {count}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_4_normal_loop(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfLJIKY-bEAA"
      },
      "source": [
        "# ***Function 5: Combine pages with same title***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L08xcGvbEh5",
        "outputId": "c3347097-02d5-488d-8651-e701450020fa"
      },
      "outputs": [],
      "source": [
        "# Function_5 - Spark Map Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_5 - Spark Loops\n",
        "def function_5_spark_loop(rdd):\n",
        "    print(\"\\nQ5 Spark foreach\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    title_groups = rdd.map(lambda x: (x[1], x)).collect()\n",
        "\n",
        "    grouped = {}\n",
        "    for title, record in title_groups:\n",
        "        if title not in grouped:\n",
        "            grouped[title] = []\n",
        "        grouped[title].append(record)\n",
        "\n",
        "    multiple_pages = {k: v for k, v in grouped.items() if len(v) > 1}\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(\"First 5 titles with multiple pages:\")\n",
        "    for i, (title, pages) in enumerate(list(multiple_pages.items())[:5]):\n",
        "        print(f\"\\nTitle: {title}\")\n",
        "        print(\"Pages:\")\n",
        "        for page in pages:\n",
        "            print(f\"  Project: {page[0]}, Hits: {page[2]}, Size: {page[3]}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_5_spark_loop(parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function_5 - Normal Loops\n",
        "def function_5_normal_loop(rdd):\n",
        "    print(\"\\nQ5 Normal Loops\")\n",
        "    start = time.time()\n",
        "    print(f\"Start Time: {start}\")\n",
        "\n",
        "    data = rdd.toLocalIterator()\n",
        "    grouped = {}\n",
        "    for x in data:\n",
        "        grouped.setdefault(x[1], []).append(x)\n",
        "    results = [(k, v) for k, v in grouped.items() if len(v) > 1][:5]\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"End Time: {end}\")\n",
        "    print(\"First 5 titles with multiple pages:\")\n",
        "    for title, pages in results:\n",
        "        print(f\"\\nTitle: {title}\")\n",
        "        print(\"Pages:\")\n",
        "        for page in pages:\n",
        "            print(f\"  Project: {page[0]}, Hits: {page[2]}, Size: {page[3]}\")\n",
        "    print(f\"Time: {end - start:.4f} seconds\")\n",
        "\n",
        "\n",
        "function_5_normal_loop(parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ***End The Spark Session***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "qQWO0XVpbnph"
      },
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
